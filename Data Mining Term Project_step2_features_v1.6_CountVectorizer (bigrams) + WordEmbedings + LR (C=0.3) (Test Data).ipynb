{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = ['twitter-2013train.txt','twitter-2015train.txt','twitter-2016train.txt']\n",
    "files = [r'C:\\Users\\Ahmed\\Desktop\\Project_Data\\twitter-2013train.txt', r'C:\\Users\\Ahmed\\Desktop\\Project_Data\\twitter-2015train.txt', r'C:\\Users\\Ahmed\\Desktop\\Project_Data\\twitter-2016train.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0, df1, df2 = [pd.read_csv(name, delimiter = '\\t', header = None) for name in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df0, df1, df2], ignore_index=True) #concatinating the tweets data in 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2], dtype='int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['serial', 'opinion', 'tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opinion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>2374</td>\n",
       "      <td>2374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>6840</td>\n",
       "      <td>6840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>6827</td>\n",
       "      <td>6827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          serial  tweet_text\n",
       "opinion                     \n",
       "negative    2374        2374\n",
       "neutral     6840        6840\n",
       "positive    6827        6827"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(by = 'opinion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16041, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>opinion</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               serial   opinion  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  Gas by my house hit $3.39!!!! I\\u2019m going t...  \n",
       "1  Theo Walcott is still shit\\u002c watch Rafa an...  \n",
       "2  its not that I\\u2019m a GSP fan\\u002c i just h...  \n",
       "3  Iranian general says Israel\\u2019s Iron Dome c...  \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Clean up & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: casefold\n",
    "\n",
    "import nltk\n",
    "\n",
    "lowerTweets =[]\n",
    "for tweet in data['tweet_text']:\n",
    "    lowerTweets.append(tweet.casefold())\n",
    "#lowerTweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ahmed\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: remove stopwords applying on all tweets \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "filtered_tweets =[]\n",
    "for doc in lowerTweets:\n",
    "    curr = \"\"\n",
    "    for word in  re.split(\"\\W+\",doc):\n",
    "        if word not in stops: \n",
    "            curr = curr + word +\" \"\n",
    "    curr = curr.strip()\n",
    "    filtered_tweets.append(curr)\n",
    "#filtered_tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove punctuation and digits from tweets and replace it by space\n",
    "#### NOTE: Through different combinations, it is observed that accuracy is decreased after removing punctuation.\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    output = []\n",
    "    for tweet in input_text:\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        output.append(tweet.translate(trantab))\n",
    "    return output\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    out_list = []\n",
    "    for j in input_text:\n",
    "        out_list.append(re.sub('\\d+', '', j))\n",
    "    return out_list\n",
    "\n",
    "punctuation_removed_tweets = remove_punctuation(filtered_tweets)\n",
    "punctuation_removed_tweets[0:5]\n",
    "\n",
    "# Here we will skip removing the punctuation,\n",
    "# and will use the \"remove_digits\" function with \"filtered_tweets\" (output of stopwords)\n",
    "digits_removed_tweets = remove_digits(filtered_tweets)\n",
    "#digits_removed_tweets[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform trimming to remove extra whitespaces:\n",
    "\n",
    "spaces_removed_tweets = []\n",
    "for j in digits_removed_tweets:\n",
    "    spaces_removed_tweets.append(\" \".join(j.split()))\n",
    "\n",
    "#spaces_removed_tweets[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: stemwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stemDocs(f_docs):\n",
    "    stemmed_docs =[]\n",
    "    for doc in  f_docs:\n",
    "        curr = \"\"\n",
    "        for word in doc.split():  \n",
    "            curr = curr + PorterStemmer().stem(word) +\" \"\n",
    "        curr = curr.strip()\n",
    "        stemmed_docs.append(curr)\n",
    "    return  stemmed_docs\n",
    "    \n",
    "stemmed_tweets = stemDocs(spaces_removed_tweets)\n",
    "#stemmed_tweets[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After multiple trials with different combinations, the highest accuracy (64.99%) is reached through the below steps:\n",
    "\n",
    "### 1- Casefolding\n",
    "### 2- Remove stopwords\n",
    "### 3- Remove digits\n",
    "### 4- Trimming (remove whitespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>opinion</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>non_stemmed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I\\u2019m going t...</td>\n",
       "      <td>ga hous hit um go chapel hill sat</td>\n",
       "      <td>gas house hit um going chapel hill sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "      <td>theo walcott still shit uc watch rafa johnni d...</td>\n",
       "      <td>theo walcott still shit uc watch rafa johnny d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "      <td>um gsp fan uc hate nick diaz ut wait februari</td>\n",
       "      <td>um gsp fan uc hate nick diaz ut wait february</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "      <td>iranian gener say israel us iron dome ut deal ...</td>\n",
       "      <td>iranian general says israel us iron dome ut de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262682041215234048</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "      <td>tehran uc mon amour obama tri establish tie mu...</td>\n",
       "      <td>tehran uc mon amour obama tried establish ties...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               serial   opinion  \\\n",
       "0  264183816548130816  positive   \n",
       "1  263405084770172928  negative   \n",
       "2  262163168678248449  negative   \n",
       "3  264249301910310912  negative   \n",
       "4  262682041215234048   neutral   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  Gas by my house hit $3.39!!!! I\\u2019m going t...   \n",
       "1  Theo Walcott is still shit\\u002c watch Rafa an...   \n",
       "2  its not that I\\u2019m a GSP fan\\u002c i just h...   \n",
       "3  Iranian general says Israel\\u2019s Iron Dome c...   \n",
       "4  Tehran\\u002c Mon Amour: Obama Tried to Establi...   \n",
       "\n",
       "                                       stemmed_tweet  \\\n",
       "0                  ga hous hit um go chapel hill sat   \n",
       "1  theo walcott still shit uc watch rafa johnni d...   \n",
       "2      um gsp fan uc hate nick diaz ut wait februari   \n",
       "3  iranian gener say israel us iron dome ut deal ...   \n",
       "4  tehran uc mon amour obama tri establish tie mu...   \n",
       "\n",
       "                                   non_stemmed_tweet  \n",
       "0             gas house hit um going chapel hill sat  \n",
       "1  theo walcott still shit uc watch rafa johnny d...  \n",
       "2      um gsp fan uc hate nick diaz ut wait february  \n",
       "3  iranian general says israel us iron dome ut de...  \n",
       "4  tehran uc mon amour obama tried establish ties...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['stemmed_tweet'] = stemmed_tweets\n",
    "data['non_stemmed_tweet'] = spaces_removed_tweets\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "accuracies = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Features extraction\n",
    "#### A. Trying Word embedding on the preprocessed Non stemmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word embedding model build\n",
    "model = KeyedVectors.load_word2vec_format('D:\\Ahmed El-Shazli\\Ahmed_Personal\\Data Science NU Diploma\\Semester 2\\CIT653 - Practical Data Mining\\Assignments\\Assignment 3\\GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the tweets to word embedding vectors using Google news W2V (300)\n",
    "vectorized_tweets = []\n",
    "for i in range(len(data.non_stemmed_tweet)):\n",
    "    #print(i)\n",
    "    tweets = []\n",
    "    words = data.non_stemmed_tweet[i].split()\n",
    "    length = len(words)\n",
    "    #print(type(words))\n",
    "    vector = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            #print(\"yes\")\n",
    "            vector += np.array(model[word])\n",
    "            #print(vector)\n",
    "        else:\n",
    "            vector += np.zeros(300)\n",
    "            #print(\"no\")\n",
    "            continue\n",
    "    #print(vector)\n",
    "    tweets = list(vector/length)\n",
    "    #print(type(tweets))\n",
    "    vectorized_tweets = np.append(vectorized_tweets, tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tweets = np.reshape(vectorized_tweets, (-1,300), 'a')\n",
    "np.shape(w2v_tweets)\n",
    "#len(vectorized_tweets)/300\n",
    "\n",
    "w2v_tweets = pd.DataFrame(w2v_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting w2v_tweets into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the provided data as training data and the remaining 30% to test a classifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "tweets_train,tweets_test,train_labels,test_labels = train_test_split(w2v_tweets,                   \n",
    "                                                 data['opinion'], test_size=0.3,\n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #will be used to scale the data between (0,1) to avoid -ve input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_tweets_train = scaler.fit_transform(tweets_train)\n",
    "scaled_tweets_test = scaler.fit_transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Non stemmed tweets into train & test and apply CountVectorizer to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the provided data as training data and the remaining 30% to test a classifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_train,tweets_test,train_labels,test_labels = train_test_split(data[\"non_stemmed_tweet\"],                   \n",
    "                                                 data['opinion'], test_size=0.3,\n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer:\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2)).fit(tweets_train)\n",
    "\n",
    "# Training Dataset:\n",
    "tweets_train_vectorized = vectorizer.transform(tweets_train)\n",
    "\n",
    "# Test Dataset:\n",
    "tweets_test_vectorized = vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Training Dataset of Countvectorizer \"tweets_train_vectorized\" & Scaled Training Dataset of Word Embeddings \"scaled_tweets_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>25285</th>\n",
       "      <th>25286</th>\n",
       "      <th>25287</th>\n",
       "      <th>25288</th>\n",
       "      <th>25289</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   25285  25286  25287  25288  25289\n",
       "0      0      0      0      0      0\n",
       "1      0      0      0      0      0\n",
       "2      0      0      0      0      0\n",
       "3      0      0      0      0      0\n",
       "4      0      0      0      0      0\n",
       "5      0      0      0      0      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_vectorized_df = pd.DataFrame(tweets_train_vectorized.toarray()) # convert scipy.sparse.csr.csr_matrix to pandas df\n",
    "#tweets_train_vectorized_df.shape\n",
    "tweets_train_vectorized_df.iloc[0:6,25285:25290] # print the last 5 columns of \"tweets_train_vectorized_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444270</td>\n",
       "      <td>0.482531</td>\n",
       "      <td>0.659303</td>\n",
       "      <td>0.415521</td>\n",
       "      <td>0.652453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.673351</td>\n",
       "      <td>0.386446</td>\n",
       "      <td>0.713009</td>\n",
       "      <td>0.402342</td>\n",
       "      <td>0.672255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.425519</td>\n",
       "      <td>0.354558</td>\n",
       "      <td>0.458270</td>\n",
       "      <td>0.779926</td>\n",
       "      <td>0.500485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.647600</td>\n",
       "      <td>0.605655</td>\n",
       "      <td>0.791132</td>\n",
       "      <td>0.558395</td>\n",
       "      <td>0.561855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.598754</td>\n",
       "      <td>0.603459</td>\n",
       "      <td>0.422123</td>\n",
       "      <td>0.467308</td>\n",
       "      <td>0.678175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.572475</td>\n",
       "      <td>0.492044</td>\n",
       "      <td>0.377940</td>\n",
       "      <td>0.405297</td>\n",
       "      <td>0.558070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        295       296       297       298       299\n",
       "0  0.444270  0.482531  0.659303  0.415521  0.652453\n",
       "1  0.673351  0.386446  0.713009  0.402342  0.672255\n",
       "2  0.425519  0.354558  0.458270  0.779926  0.500485\n",
       "3  0.647600  0.605655  0.791132  0.558395  0.561855\n",
       "4  0.598754  0.603459  0.422123  0.467308  0.678175\n",
       "5  0.572475  0.492044  0.377940  0.405297  0.558070"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaled_tweets_train_df = DataFrame(data=scaled_tweets_train, index=scaled_tweets_train.index)\n",
    "scaled_tweets_train_df = pd.DataFrame(data=scaled_tweets_train)\n",
    "scaled_tweets_train_df.head()\n",
    "scaled_tweets_train_df.iloc[0:6,295:300] # print the last 5 columns of \"scaled_tweets_train_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11228, 124144)\n",
      "(11228, 300)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_train_vectorized_df.shape)\n",
    "print(scaled_tweets_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>124434</th>\n",
       "      <th>124435</th>\n",
       "      <th>124436</th>\n",
       "      <th>124437</th>\n",
       "      <th>124438</th>\n",
       "      <th>124439</th>\n",
       "      <th>124440</th>\n",
       "      <th>124441</th>\n",
       "      <th>124442</th>\n",
       "      <th>124443</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387537</td>\n",
       "      <td>0.503434</td>\n",
       "      <td>0.640779</td>\n",
       "      <td>0.508037</td>\n",
       "      <td>0.586197</td>\n",
       "      <td>0.444270</td>\n",
       "      <td>0.482531</td>\n",
       "      <td>0.659303</td>\n",
       "      <td>0.415521</td>\n",
       "      <td>0.652453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383731</td>\n",
       "      <td>0.579296</td>\n",
       "      <td>0.673932</td>\n",
       "      <td>0.483961</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.673351</td>\n",
       "      <td>0.386446</td>\n",
       "      <td>0.713009</td>\n",
       "      <td>0.402342</td>\n",
       "      <td>0.672255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438375</td>\n",
       "      <td>0.708575</td>\n",
       "      <td>0.528595</td>\n",
       "      <td>0.449150</td>\n",
       "      <td>0.348899</td>\n",
       "      <td>0.425519</td>\n",
       "      <td>0.354558</td>\n",
       "      <td>0.458270</td>\n",
       "      <td>0.779926</td>\n",
       "      <td>0.500485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363190</td>\n",
       "      <td>0.446790</td>\n",
       "      <td>0.619325</td>\n",
       "      <td>0.476633</td>\n",
       "      <td>0.575123</td>\n",
       "      <td>0.647600</td>\n",
       "      <td>0.605655</td>\n",
       "      <td>0.791132</td>\n",
       "      <td>0.558395</td>\n",
       "      <td>0.561855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382165</td>\n",
       "      <td>0.649230</td>\n",
       "      <td>0.653916</td>\n",
       "      <td>0.400232</td>\n",
       "      <td>0.522325</td>\n",
       "      <td>0.598754</td>\n",
       "      <td>0.603459</td>\n",
       "      <td>0.422123</td>\n",
       "      <td>0.467308</td>\n",
       "      <td>0.678175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 124444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0       1       2       3       4       5       6       7       8       \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   9         ...       124434    124435    124436    124437    124438  \\\n",
       "0       0    ...     0.387537  0.503434  0.640779  0.508037  0.586197   \n",
       "1       0    ...     0.383731  0.579296  0.673932  0.483961  0.364865   \n",
       "2       0    ...     0.438375  0.708575  0.528595  0.449150  0.348899   \n",
       "3       0    ...     0.363190  0.446790  0.619325  0.476633  0.575123   \n",
       "4       0    ...     0.382165  0.649230  0.653916  0.400232  0.522325   \n",
       "\n",
       "     124439    124440    124441    124442    124443  \n",
       "0  0.444270  0.482531  0.659303  0.415521  0.652453  \n",
       "1  0.673351  0.386446  0.713009  0.402342  0.672255  \n",
       "2  0.425519  0.354558  0.458270  0.779926  0.500485  \n",
       "3  0.647600  0.605655  0.791132  0.558395  0.561855  \n",
       "4  0.598754  0.603459  0.422123  0.467308  0.678175  \n",
       "\n",
       "[5 rows x 124444 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging tweets_train_vectorized_df & scaled_tweets_train_df:\n",
    "\n",
    "df_concat_train = pd.concat([tweets_train_vectorized_df, scaled_tweets_train_df], axis=1, ignore_index=True)\n",
    "df_concat_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25285  25286  25287  25288  25289\n",
      "0      0      0      0      0      0\n",
      "1      0      0      0      0      0\n",
      "2      0      0      0      0      0\n",
      "3      0      0      0      0      0\n",
      "4      0      0      0      0      0\n",
      "5      0      0      0      0      0\n",
      "   25585  25586  25587  25588  25589\n",
      "0      0      0      0      0      0\n",
      "1      0      0      0      0      0\n",
      "2      0      0      0      0      0\n",
      "3      0      0      0      0      0\n",
      "4      0      0      0      0      0\n",
      "5      0      0      0      0      0\n"
     ]
    }
   ],
   "source": [
    "print(df_concat_train.iloc[0:6,25285:25290])   # print the last 5 columns of \"tweets_train_vectorized_df\" after merging\n",
    "print(df_concat_train.iloc[0:6,25585:25590])   # print the last 5 columns of \"scaled_tweets_train_df\" after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>25285</th>\n",
       "      <th>25286</th>\n",
       "      <th>25287</th>\n",
       "      <th>25288</th>\n",
       "      <th>25289</th>\n",
       "      <th>25290</th>\n",
       "      <th>25291</th>\n",
       "      <th>25292</th>\n",
       "      <th>25293</th>\n",
       "      <th>25294</th>\n",
       "      <th>25295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   25285  25286  25287  25288  25289  25290  25291  25292  25293  25294  25295\n",
       "0      0      0      0      0      0      0      0      0      0      0      0\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2      0      0      0      0      0      0      0      0      0      0      0\n",
       "3      0      0      0      0      0      0      0      0      0      0      0\n",
       "4      0      0      0      0      0      0      0      0      0      0      0\n",
       "5      0      0      0      0      0      0      0      0      0      0      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_tweets_train_df.iloc[0:6,0:6]   # print the first 5 columns of \"scaled_tweets_train_df\" BEFORE merge\n",
    "df_concat_train.iloc[0:6,25285:25296]   # print the first 5 columns of \"scaled_tweets_train_df\" AFTER merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Test Dataset of Countvectorizer \"tweets_test_vectorized\" & Scaled Test Dataset of Word Embeddings \"scaled_tweets_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging tweets_test_vectorized_df & scaled_test_train_df:\n",
    "\n",
    "#tweets_test_vectorized_df = pd.DataFrame(tweets_test_vectorized.toarray())\n",
    "#scaled_tweets_test_df = pd.DataFrame(data=scaled_tweets_test)\n",
    "\n",
    "#print(tweets_test_vectorized_df.shape)\n",
    "#print(scaled_tweets_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_tweets_test_df.iloc[0:6,295:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_concat_test = pd.concat([tweets_test_vectorized_df, scaled_tweets_test_df], axis=1, ignore_index=True)\n",
    "#df_concat_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Merged data with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#clfr = LogisticRegression(C=0.3359818286283781)\n",
    "#clfr.fit(df_concat_train,train_labels)\n",
    "\n",
    "#predicted = clfr.predict(df_concat_test)\n",
    "#acc = metrics.accuracy_score(test_labels,predicted)\n",
    "\n",
    "#print ('Accuracy of Merged Data (Word Embdeddings & CountVectorizer) + Logistic Regression (Non Stemmed Tweets) = '+str(acc*100)+'%')\n",
    "#print (metrics.classification_report(test_labels,predicted))\n",
    "#accuracies.append(('Accuracy of Merged Data (Word Embdeddings & CountVectorizer) + Logistic Regression (Non Stemmed Tweets)', acc*100))\n",
    "#f1_scores.append(('F1-score of Merged Data (Word Embdeddings & CountVectorizer) + Logistic Regression (Non Stemmed Tweets)', metrics.f1_score(test_labels,predicted, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the provided Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files_test = [r'D:\\Ahmed El-Shazli\\Ahmed_Personal\\Data Science NU Diploma\\Semester 2\\CIT653 - Practical Data Mining\\Project\\Test Data\\test.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Ahmed El-Shazli\\\\Ahmed_Personal\\\\Data Science NU Diploma\\\\Semester 2\\\\CIT653 - Practical Data Mining\\\\Project\\\\Test Data\\\\test.csv']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(r'D:\\Ahmed El-Shazli\\Ahmed_Personal\\Data Science NU Diploma\\Semester 2\\CIT653 - Practical Data Mining\\Project\\Test Data\\test.csv')\n",
    "#df3, df4 = [pd.read_csv(name, delimiter = '\\t', header = None) for name in files_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218775148495515649</td>\n",
       "      <td>' Musical awareness: Great Big Beautiful Tomor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258965201766998017</td>\n",
       "      <td>' On Radio786 100.4fm 7:10 Fri Oct 19 Labour a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262926411352903682</td>\n",
       "      <td>' Kapan sih lo ngebuktiin,jan ngomong doang Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254948834910818305</td>\n",
       "      <td>' Tomorrow come and hear @DavidWillettsMP&amp;amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171874368908050432</td>\n",
       "      <td>' Excuse the connectivity of this live stream,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                              tweet\n",
       "0  218775148495515649  ' Musical awareness: Great Big Beautiful Tomor...\n",
       "1  258965201766998017  ' On Radio786 100.4fm 7:10 Fri Oct 19 Labour a...\n",
       "2  262926411352903682  ' Kapan sih lo ngebuktiin,jan ngomong doang Su...\n",
       "3  254948834910818305  ' Tomorrow come and hear @DavidWillettsMP&amp;...\n",
       "4  171874368908050432  ' Excuse the connectivity of this live stream,..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3096, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet'], dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Clean Up & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"' musical awareness: great big beautiful tomorrow has an ending, now is the time does not \",\n",
       " \"' on radio786 100.4fm 7:10 fri oct 19 labour analyst shawn hattingh: cosatu\\\\'s role in the context of unrest in the mining http://t.co/46pjzzl6 \",\n",
       " \"' kapan sih lo ngebuktiin,jan ngomong doang susah susah.usaha aja blm udh nyerah,inget.if you never try you\\\\'ll never know.cowok kok gentle bgt \",\n",
       " '\\' tomorrow come and hear @davidwillettsmp&amp;@masieghart debate \"navigating the new higher education market\" 5.30pm, jurys inn #cpc12 ',\n",
       " \"' excuse the connectivity of this live stream, from baba amr, so many activists using only one sat modem. live http://t.co/u283ihz5 #homs \"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: casefold\n",
    "\n",
    "import nltk\n",
    "\n",
    "lowerTweets_test =[]\n",
    "for tweet in data_test['tweet']:\n",
    "    lowerTweets_test.append(tweet.casefold())\n",
    "#lowerTweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: remove stopwords applying on all tweets \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "filtered_tweets_test =[]\n",
    "for doc in lowerTweets_test:\n",
    "    curr = \"\"\n",
    "    for word in  re.split(\"\\W+\",doc):\n",
    "        if word not in stops: \n",
    "            curr = curr + word +\" \"\n",
    "    curr = curr.strip()\n",
    "    filtered_tweets_test.append(curr)\n",
    "#filtered_tweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove punctuation and digits from tweets and replace it by space\n",
    "#### NOTE: Through different combinations, it is observed that accuracy is decreased after removing punctuation.\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    output = []\n",
    "    for tweet in input_text:\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        output.append(tweet.translate(trantab))\n",
    "    return output\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    out_list = []\n",
    "    for j in input_text:\n",
    "        out_list.append(re.sub('\\d+', '', j))\n",
    "    return out_list\n",
    "\n",
    "#punctuation_removed_tweets = remove_punctuation(filtered_tweets)\n",
    "#punctuation_removed_tweets[0:5]\n",
    "\n",
    "# Here we will skip removing the punctuation,\n",
    "# and will use the \"remove_digits\" function with \"filtered_tweets\" (output of stopwords)\n",
    "digits_removed_tweets_test = remove_digits(filtered_tweets_test)\n",
    "#digits_removed_tweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform trimming to remove extra whitespaces:\n",
    "\n",
    "spaces_removed_tweets_test = []\n",
    "for j in digits_removed_tweets_test:\n",
    "    spaces_removed_tweets_test.append(\" \".join(j.split()))\n",
    "\n",
    "#spaces_removed_tweets_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocessed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218775148495515649</td>\n",
       "      <td>' Musical awareness: Great Big Beautiful Tomor...</td>\n",
       "      <td>musical awareness great big beautiful tomorrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258965201766998017</td>\n",
       "      <td>' On Radio786 100.4fm 7:10 Fri Oct 19 Labour a...</td>\n",
       "      <td>radio fm fri oct labour analyst shawn hattingh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262926411352903682</td>\n",
       "      <td>' Kapan sih lo ngebuktiin,jan ngomong doang Su...</td>\n",
       "      <td>kapan sih lo ngebuktiin jan ngomong doang susa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254948834910818305</td>\n",
       "      <td>' Tomorrow come and hear @DavidWillettsMP&amp;amp;...</td>\n",
       "      <td>tomorrow come hear davidwillettsmp amp masiegh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171874368908050432</td>\n",
       "      <td>' Excuse the connectivity of this live stream,...</td>\n",
       "      <td>excuse connectivity live stream baba amr many ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                              tweet  \\\n",
       "0  218775148495515649  ' Musical awareness: Great Big Beautiful Tomor...   \n",
       "1  258965201766998017  ' On Radio786 100.4fm 7:10 Fri Oct 19 Labour a...   \n",
       "2  262926411352903682  ' Kapan sih lo ngebuktiin,jan ngomong doang Su...   \n",
       "3  254948834910818305  ' Tomorrow come and hear @DavidWillettsMP&amp;...   \n",
       "4  171874368908050432  ' Excuse the connectivity of this live stream,...   \n",
       "\n",
       "                                  preprocessed_tweet  \n",
       "0  musical awareness great big beautiful tomorrow...  \n",
       "1  radio fm fri oct labour analyst shawn hattingh...  \n",
       "2  kapan sih lo ngebuktiin jan ngomong doang susa...  \n",
       "3  tomorrow come hear davidwillettsmp amp masiegh...  \n",
       "4  excuse connectivity live stream baba amr many ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['stemmed_tweet'] = stemmed_tweets\n",
    "data_test['preprocessed_tweet'] = spaces_removed_tweets_test\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply both CountVectorizer & Word Embeddings on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Applying CountVectorizer Provided Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply CountVectorizer on Test Dataset:\n",
    "\n",
    "new_tweets_test_vectorized = vectorizer.transform(data_test['preprocessed_tweet'])\n",
    "#new_tweets_test_vectorized\n",
    "#vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Applying W2V on Provided Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the w2v on the TEST TWEETS to word embedding vectors using Google news W2V (300):\n",
    "\n",
    "vectorized_test_tweets = []\n",
    "for i in range(len(data_test.preprocessed_tweet)):\n",
    "    #print(i)\n",
    "    tweets = []\n",
    "    words = data_test.preprocessed_tweet[i].split()\n",
    "    length = len(words)\n",
    "    #print(type(words))\n",
    "    vector = 0\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            #print(\"yes\")\n",
    "            vector += np.array(model[word])\n",
    "            #print(vector)\n",
    "        else:\n",
    "            vector += np.zeros(300)\n",
    "            #print(\"no\")\n",
    "            continue\n",
    "    #print(vector)\n",
    "    tweets = list(vector/length)\n",
    "    #print(type(tweets))\n",
    "    vectorized_test_tweets = np.append(vectorized_test_tweets, tweets)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_test_tweets = np.reshape(vectorized_test_tweets, (-1,300), 'a')\n",
    "np.shape(w2v_test_tweets)\n",
    "#len(vectorized_tweets)/300\n",
    "\n",
    "w2v_test_tweets = pd.DataFrame(w2v_test_tweets)\n",
    "#w2v_test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling w2v_test_tweets:\n",
    "scaler = MinMaxScaler()\n",
    "scaled_test_tweets = scaler.fit_transform(w2v_test_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Merging Vectorized Test Dataset of Countvectorizer \"new_tweets_test_vectorized\" & Scaled Test Dataset of Word Embeddings \"scaled_test_tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweets_test_vectorized_df = pd.DataFrame(new_tweets_test_vectorized.toarray()) # convert scipy.sparse.csr.csr_matrix to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_tweets_df = pd.DataFrame(data=scaled_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-f1475f4bb00b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Merging new_tweets_test_vectorized_df & scaled_test_tweets_df:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_merged_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_tweets_test_vectorized_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_test_tweets_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#df_merged_test.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 226\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5410\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5411\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5412\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5413\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5414\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Merging new_tweets_test_vectorized_df & scaled_test_tweets_df:\n",
    "\n",
    "df_merged_test = pd.concat([new_tweets_test_vectorized_df, scaled_test_tweets_df], axis=1, ignore_index=True)\n",
    "#df_merged_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0       1       2       3       4       5       6       7       8       \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   9         ...       124434    124435    124436    124437    124438  \\\n",
      "0       0    ...     0.387537  0.503434  0.640779  0.508037  0.586197   \n",
      "1       0    ...     0.383731  0.579296  0.673932  0.483961  0.364865   \n",
      "2       0    ...     0.438375  0.708575  0.528595  0.449150  0.348899   \n",
      "3       0    ...     0.363190  0.446790  0.619325  0.476633  0.575123   \n",
      "4       0    ...     0.382165  0.649230  0.653916  0.400232  0.522325   \n",
      "\n",
      "     124439    124440    124441    124442    124443  \n",
      "0  0.444270  0.482531  0.659303  0.415521  0.652453  \n",
      "1  0.673351  0.386446  0.713009  0.402342  0.672255  \n",
      "2  0.425519  0.354558  0.458270  0.779926  0.500485  \n",
      "3  0.647600  0.605655  0.791132  0.558395  0.561855  \n",
      "4  0.598754  0.603459  0.422123  0.467308  0.678175  \n",
      "\n",
      "[5 rows x 124444 columns]\n",
      "13146    positive\n",
      "6931     negative\n",
      "11918    positive\n",
      "4774      neutral\n",
      "10756    negative\n",
      "Name: opinion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df_concat_train.head())\n",
    "#print(train_labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Logistic Regression on the Megred Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression classifier:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "clfr = LogisticRegression(C=0.3359818286283781)\n",
    "clfr.fit(df_concat_train,train_labels)\n",
    "\n",
    "predicted_test = clfr.predict(df_merged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test\n",
    "print(len(predicted_test))\n",
    "print(predicted_test[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_list = []\n",
    "for prediction in range(len(predicted_test)):\n",
    "    if predicted_test[prediction] == 'neutral':\n",
    "        label_list.append(0)\n",
    "    if predicted_test[prediction] == 'positive':\n",
    "        label_list.append(1)\n",
    "    if predicted_test[prediction] == 'negative':\n",
    "        label_list.append(2)\n",
    "\n",
    "print(len(label_list))\n",
    "print(label_list[0:11])\n",
    "\n",
    "label_array = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_test[['id']].copy()\n",
    "result['label'] = pd.DataFrame(data=label_array)\n",
    "result.head(10)\n",
    "result.to_csv(r\"C:\\Users\\Ahmed\\Desktop\\output_third_trial_w2v+bigrams+LR.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
